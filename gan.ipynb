{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bitdaca1fc0f09a4ba6b28451dc7aeb19e7",
   "display_name": "Python 3.7.6 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n"
   ]
  },
  {
   "source": [
    "# Constants and datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_dim = 100\n",
    "\n",
    "# Gans are very sensitive to hyperparameters, choosing right parameters for\n",
    "# the optimizer can make the difference between learning and not learning.\n",
    "optimizer = tf.keras.optimizers.Adam(0.0005,0.6)\n",
    "\n",
    "batch_size = 128"
   ]
  },
  {
   "source": [
    "Few notable things:\n",
    "\n",
    "- we only need the X_train data\n",
    "- the labels are scaled to stay between -1 and 1 (using an hyperbolic tangent with the output layer seems to give better results)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, _), (_, _) = mnist.load_data()\n",
    "X_train = X_train / 127.5 - 1.\n",
    "X_train = X_train.reshape(-1,28,28,1)"
   ]
  },
  {
   "source": [
    "# Generator definition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_in = Input(shape=[z_dim])\n",
    "x = Dense(1000, activation=\"relu\")(G_in)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(512, activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(784, activation=\"tanh\")(x)\n",
    "G_out = Reshape([28,28,1])(x)\n",
    "G = Model(G_in, G_out, name=\"G\")"
   ]
  },
  {
   "source": [
    "# Discriminator definition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_in = Input(shape=[28,28,1])\n",
    "x = Flatten()(D_in)\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "x = Dropout(0.25)(x)\n",
    "x = Dense(128, activation=\"sigmoid\")(x)\n",
    "x = Dropout(0.25)(x)\n",
    "D_out = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "D = Model(D_in, D_out, name=\"D\")\n",
    "\n",
    "# Let's keep under control few important metrics (tp and tn in particular)\n",
    "D.compile(loss=\"binary_crossentropy\", optimizer=optimizer, \n",
    "    metrics=[\"accuracy\",tf.keras.metrics.TruePositives(),tf.keras.metrics.TrueNegatives()])"
   ]
  },
  {
   "source": [
    "# GAN definition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_in = Input(shape=[z_dim])\n",
    "gan_out = D(G(gan_in))\n",
    "gan = Model(gan_in, gan_out)\n",
    "\n",
    "gan.compile(loss=\"binary_crossentropy\", optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.summary()"
   ]
  },
  {
   "source": [
    "# Few utility functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_z(gan, num):\n",
    "    G = gan.layers[1]\n",
    "    z_dim = G.layers[0].input.shape[1]\n",
    "    return np.random.normal(0,1,size=[num,z_dim])\n",
    "\n",
    "def get_real_imgs(ds, num):\n",
    "    ndx = np.random.randint(0,len(ds), size=num)\n",
    "    return ds[ndx]\n",
    "\n",
    "def get_gen_imgs(gan, num):\n",
    "    zs = generate_random_z(gan, num)\n",
    "    G = gan.layers[1]\n",
    "    return G.predict(zs)\n",
    "\n",
    "def show_img_samples(gan, step_no):\n",
    "    D = gan.layers[2]\n",
    "    imgs = get_gen_imgs(gan, 25)\n",
    "    pred_fake = D.predict(imgs) < 0.5\n",
    "\n",
    "    fig, axes = plt.subplots(5, 5, figsize=(20,20))\n",
    "    cmaps = { True: \"hot\", False: \"gray\" }\n",
    "\n",
    "    for i in range(5):\n",
    "        for j in range(5):\n",
    "            imgno = i*5 + j\n",
    "            ax = axes[i,j]\n",
    "            ax.imshow(imgs[imgno], cmap=cmaps[pred_fake[imgno][0]])\n",
    "\n",
    "    plt.savefig(\"images/img_{}.png\".format(step_no))"
   ]
  },
  {
   "source": [
    "# Main train loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def D_train(ds, gan, batch_size):\n",
    "    D = gan.layers[2]\n",
    "    D.trainable = True\n",
    "\n",
    "    for i in range(5):\n",
    "        x_real = get_real_imgs(ds, batch_size)\n",
    "        x_gen  = get_gen_imgs(gan, batch_size)\n",
    "\n",
    "        y_real = np.ones([batch_size])\n",
    "        y_gen  = np.zeros([batch_size])\n",
    "\n",
    "        x = np.concatenate([x_real, x_gen])\n",
    "        y = np.concatenate([y_real, y_gen])\n",
    "\n",
    "        perfs = D.train_on_batch(x,y)\n",
    "\n",
    "    return perfs\n",
    "\n",
    "\n",
    "def G_train(ds, gan, batch_size):\n",
    "    D = gan.layers[2]\n",
    "    D.trainable = False\n",
    "\n",
    "    zs = generate_random_z(gan, batch_size*2)\n",
    "    y = np.ones([batch_size*2])\n",
    "    return gan.train_on_batch(zs, y)\n",
    "\n",
    "\n",
    "def train(gan, ds, steps, batch_size=batch_size):\n",
    "\n",
    "    for i in range(steps):\n",
    "\n",
    "        d_loss, d_acc, d_tp, d_tn = D_train(ds, gan, batch_size)\n",
    "        g_loss = G_train(ds, gan, batch_size)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(\"\\ri:{} G[l:{:0.3f}] D[l:{:0.3f} a:{:0.3f} +:{:0.3f} -:{:0.3f}]\"\n",
    "                .format(i,    g_loss,      d_loss,    d_acc,     d_tp,     d_tn), end=\"\")\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            show_img_samples(gan, i)\n",
    "\n",
    "    show_img_samples(gan, steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(gan, X_train, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}